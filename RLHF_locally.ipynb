{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a069b0",
   "metadata": {},
   "source": [
    "```\n",
    "Authors: Ehsan Kamalinejad (EK), Emily Webber\n",
    "Created: 2023-02-27\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757937bc",
   "metadata": {},
   "source": [
    "# RLHF\n",
    "\n",
    "Reinforcement learning with human feedback is an interesting technique designed to aggregate human prefences at scale. In particular we train a regressive large language model built on human-desginated ranks for each prompt. Then, we use this model to serve as the `reward signal`, fine-tuning another LLM. \n",
    "\n",
    "Here, we present a training pipeline to finetune a generative model to create IMDb reviews with positive sentiment according to the [OpenAI RLHF paper](https://arxiv.org/abs/1909.08593)(please see section 3).\n",
    "\n",
    "Please note, all of the training in this notebook currently happens locally, so you'll want to use an instance with enough accelerator memory. I'm running on a `ml.g4dn.xlarge`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b798a315-b134-4607-8a38-3af59329f2e7",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f023c01-279f-483e-8964-b0525bb33200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "tqdm\n",
    "omegaconf\n",
    "dataclasses\n",
    "torchtyping\n",
    "datasets\n",
    "transformers\n",
    "torch\n",
    "xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21bc78ef-fdce-4f01-90d2-536ce6dd8df2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (4.62.3)\n",
      "Collecting omegaconf (from -r requirements.txt (line 2))\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dataclasses (from -r requirements.txt (line 3))\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Collecting torchtyping (from -r requirements.txt (line 4))\n",
      "  Downloading torchtyping-0.1.4-py3-none-any.whl (17 kB)\n",
      "Collecting datasets (from -r requirements.txt (line 5))\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers (from -r requirements.txt (line 6))\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/d1/3bba59606141ae808017f6fde91453882f931957f125009417b87a281067/transformers-4.34.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch (from -r requirements.txt (line 7))\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/e1/24/f7fe3fe82583e6891cc3fceeb390f192f6c7f1d87e5a99a949ed33c96167/torch-2.1.0-cp38-cp38-manylinux1_x86_64.whl.metadata\n",
      "  Downloading torch-2.1.0-cp38-cp38-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting xformers (from -r requirements.txt (line 8))\n",
      "  Obtaining dependency information for xformers from https://files.pythonhosted.org/packages/41/02/95d6f0abe371271941bfda567f5d062e82013f80925253fe475e3abac5b7/xformers-0.0.22-cp38-cp38-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading xformers-0.0.22-cp38-cp38-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->-r requirements.txt (line 2))\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.8/site-packages (from omegaconf->-r requirements.txt (line 2)) (6.0)\n",
      "Collecting typeguard>=2.11.1 (from torchtyping->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for typeguard>=2.11.1 from https://files.pythonhosted.org/packages/18/01/5fc45558268ced46d86292763477996a3cdd505567cd590a688e8cdc386e/typeguard-4.1.5-py3-none-any.whl.metadata\n",
      "  Downloading typeguard-4.1.5-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 5)) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 5)) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 5)) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 5)) (1.3.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 5)) (2.31.0)\n",
      "Collecting xxhash (from datasets->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/ad/80/8fc9a4d76b259c901f2c85ed10f330a8fb51993a577bddfd53a852595e12/xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 5)) (0.70.15)\n",
      "Collecting fsspec[http]<2023.9.0,>=2023.1.0 (from datasets->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for fsspec[http]<2023.9.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e3/bd/4c0a4619494188a9db5d77e2100ab7d544a42e76b2447869d8e124e981d8/fsspec-2023.6.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting aiohttp (from datasets->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/84/22/20c41a698eb5435ff7cd3bc6884542d2a40f3c91a221f713d77217facc39/aiohttp-3.8.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.8.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for huggingface-hub<1.0.0,>=0.14.0 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 5)) (21.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 6)) (3.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 6)) (2021.8.3)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/59/15/c60dae8646210e148e8432fbb5a13d1f6fa8cefda6314ff6c4fc0b58b6ec/tokenizers-0.14.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.14.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/21/12/d95158b4fdd0422faf019038be0be874d7bf3d9f9bd0b1b529f73853cec2/safetensors-0.3.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.3.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->-r requirements.txt (line 7)) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.8/site-packages (from torch->-r requirements.txt (line 7)) (1.9)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from torch->-r requirements.txt (line 7)) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from torch->-r requirements.txt (line 7)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.8/site-packages (from torch->-r requirements.txt (line 7)) (2021.8.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->-r requirements.txt (line 7))\n",
      "  Obtaining dependency information for nvidia-cudnn-cu12==8.9.2.26 from https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.1.0 (from torch->-r requirements.txt (line 7))\n",
      "  Obtaining dependency information for triton==2.1.0 from https://files.pythonhosted.org/packages/72/98/34f43ed68ee6455ea874f749a5515c0600243186301ecd83819d942ce08a/triton-2.1.0-0-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading triton-2.1.0-0-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 7))\n",
      "  Obtaining dependency information for nvidia-nvjitlink-cu12 from https://files.pythonhosted.org/packages/0a/f8/5193b57555cbeecfdb6ade643df0d4218cc6385485492b6e2f64ceae53bb/nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting torch (from -r requirements.txt (line 7))\n",
      "  Downloading torch-2.0.1-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m959.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0 (from torch->-r requirements.txt (line 7))\n",
      "  Downloading triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 7)) (68.2.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 7)) (0.41.2)\n",
      "Collecting cmake (from triton==2.0.0->torch->-r requirements.txt (line 7))\n",
      "  Obtaining dependency information for cmake from https://files.pythonhosted.org/packages/72/89/b1cf3cd5fb9f4ae796dd4a743412553f884dad2acbf6b9828d3a0c2b5524/cmake-3.27.6-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading cmake-3.27.6-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lit (from triton==2.0.0->torch->-r requirements.txt (line 7))\n",
      "  Downloading lit-17.0.2.tar.gz (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.6/154.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (2.0.4)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 5))\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 5))\n",
      "  Downloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.9/266.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/0b/36/c276486f89bee9098332710af2207344f360c6c6f104a4931a7566039b1d/frozenlist-1.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 5))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets->-r requirements.txt (line 5)) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 5)) (3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 5)) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 5)) (2023.7.22)\n",
      "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for huggingface-hub<1.0.0,>=0.14.0 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /opt/conda/lib/python3.8/site-packages (from typeguard>=2.11.1->torchtyping->-r requirements.txt (line 4)) (4.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->torch->-r requirements.txt (line 7)) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2021.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.8/site-packages (from sympy->torch->-r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=3.6->typeguard>=2.11.1->torchtyping->-r requirements.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets->-r requirements.txt (line 5)) (1.16.0)\n",
      "Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.22-cp38-cp38-manylinux2014_x86_64.whl (211.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.8.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.3.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.14.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-4.1.5-py3-none-any.whl (34 kB)\n",
      "Downloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/194.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.1/220.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cmake-3.27.6-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime, lit\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=a2128c95e2e3bfdd9ed633ed5417e895ed803f18596a433b1e574f6e986bc673\n",
      "  Stored in directory: /root/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-17.0.2-py3-none-any.whl size=93257 sha256=b06cc94c880b47e892df0568c5567fba7579c616df1d1284f1d03e50f88c3345\n",
      "  Stored in directory: /root/.cache/pip/wheels/4d/5c/78/f646b350f97bfcf3df43d6e1afb5234ff13738da5fbdf67e97\n",
      "Successfully built antlr4-python3-runtime lit\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: safetensors, lit, dataclasses, cmake, antlr4-python3-runtime, xxhash, omegaconf, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, multidict, fsspec, frozenlist, async-timeout, yarl, typeguard, nvidia-cusolver-cu11, nvidia-cudnn-cu11, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets, triton, torch, xformers, torchtyping\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.8.1\n",
      "    Uninstalling fsspec-2021.8.1:\n",
      "      Successfully uninstalled fsspec-2021.8.1\n",
      "Successfully installed aiohttp-3.8.5 aiosignal-1.3.1 antlr4-python3-runtime-4.9.3 async-timeout-4.0.3 cmake-3.27.6 dataclasses-0.6 datasets-2.14.5 frozenlist-1.4.0 fsspec-2023.6.0 huggingface-hub-0.16.4 lit-17.0.2 multidict-6.0.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 omegaconf-2.3.0 safetensors-0.3.3 tokenizers-0.14.0 torch-2.0.1 torchtyping-0.1.4 transformers-4.34.0 triton-2.0.0 typeguard-4.1.5 xformers-0.0.22 xxhash-3.4.1 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r  requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806219f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from omegaconf import DictConfig\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "from typing import Iterable, Sequence, List\n",
    "\n",
    "from torchtyping import TensorType\n",
    "\n",
    "import transformers\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf75c75-d19d-474c-aa0c-5292c5d7b530",
   "metadata": {},
   "source": [
    "### 2. Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f63f4904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'train': {\n",
    "        'seed': 2023,\n",
    "        'seq_length': 1024,\n",
    "        'epochs': 50,\n",
    "        'total_steps': 5000,\n",
    "        'batch_size': 64,\n",
    "        'eval_interval': 100,\n",
    "        'model_device':'cuda:0',\n",
    "        'ref_model_device':'cpu',\n",
    "        'reward_model_device':'cpu'},\n",
    "    'model': {\n",
    "        'model_path': 'lvwerra/gpt2-imdb', #'edbeeching/gpt-neo-1.3B-imdb',\n",
    "        'tokenizer_path': 'lvwerra/gpt2-imdb', #'edbeeching/gpt-neo-1.3B-imdb',\n",
    "        'num_layers_unfrozen': 1},\n",
    "    'optimizer': {\n",
    "        'name': 'adamw',\n",
    "        'kwargs': {'lr': 0.0001,\n",
    "        'betas': [0.9, 0.95],\n",
    "        'eps': 1e-08,\n",
    "        'weight_decay': 1e-06}},\n",
    "    'scheduler': {\n",
    "        'name': 'cosine_annealing',\n",
    "        'kwargs': {\n",
    "            'T_max': 10000, 'eta_min': 0.0001}},\n",
    "    'method': {\n",
    "        'use_whitening': True,\n",
    "        'prompt_size': 10,\n",
    "        'num_rollouts': 128,\n",
    "        'chunk_size': 128,\n",
    "        'ppo_epochs': 4,\n",
    "        'kl_coef': 0.05,\n",
    "        'horizon': 10000,\n",
    "        'gamma': 1,\n",
    "        'lam': 0.95,\n",
    "        'cliprange': 0.2,\n",
    "        'cliprange_value': 0.2,\n",
    "        'vf_coef': 1,\n",
    "        'scale_reward': False,\n",
    "        'ref_mean': None,\n",
    "        'ref_std': None,\n",
    "        'cliprange_reward': 10,\n",
    "        'gen_kwargs': {\n",
    "            'max_new_tokens': 60,\n",
    "            'top_k': 0,\n",
    "            'top_p': 1.0,\n",
    "            'do_sample': True}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf8d10c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = DictConfig(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90ff010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(config.train.seed)\n",
    "np.random.seed(config.train.seed)\n",
    "torch.manual_seed(config.train.seed)\n",
    "torch.cuda.manual_seed(config.train.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c99cfbc-acb7-4b82-abd9-cb9bfe56e7a6",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Define PyTorch objects\n",
    "Here, we'll show you how to implement the following reward modelling objects in PyTorch:\n",
    "1. Prompt Pipeline\n",
    "2. PPO RL Element\n",
    "3. PPO RL Batch\n",
    "4. PPO Rollout Storage, including a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2080a70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PromptPipeline():\n",
    "    def __init__(self, prompts: List[str], max_prompt_length: int, tokenizer):\n",
    "        super().__init__()\n",
    "\n",
    "        prompts = tokenizer(prompts).input_ids\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = [prompt[-max_prompt_length:] for prompt in prompts]\n",
    "        self.prompts = [{\"input_ids\": prompt, \"attention_mask\": [1] * len(prompt)} for prompt in self.prompts]\n",
    "\n",
    "    def __getitem__(self, ix: int):\n",
    "        return self.prompts[ix]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def create_loader(self, batch_size: int, shuffle=False) -> DataLoader:\n",
    "        collate_fn = DataCollatorWithPadding(self.tokenizer)\n",
    "        return DataLoader(self, batch_size=batch_size, collate_fn=collate_fn, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e497c3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PPORLElement:\n",
    "    query_tensor: TensorType[\"query_size\"]\n",
    "    response_tensor: TensorType[\"response_size\"]\n",
    "    logprobs: TensorType[\"response_size\", \"vocab_size\"]\n",
    "    values: TensorType[\"response_size\"]\n",
    "    rewards: TensorType[\"response_size\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PPORLBatch:\n",
    "    query_tensors: TensorType[\"batch_size\", \"query_size\"]\n",
    "    response_tensors: TensorType[\"batch_size\", \"response_size\"]\n",
    "    logprobs: TensorType[\"batch_size\", \"response_size\", \"vocab_size\"]\n",
    "    values: TensorType[\"batch_size\", \"response_size\"]\n",
    "    rewards: TensorType[\"batch_size\", \"response_size\"]\n",
    "\n",
    "\n",
    "class PPORolloutStorage():\n",
    "    def __init__(self, pad_token_id):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.history: Iterable[PPORLElement] = [None]\n",
    "\n",
    "    def push(self, exps: Iterable[PPORLElement]):\n",
    "        self.history += exps\n",
    "\n",
    "    def clear_history(self):\n",
    "        self.history = []\n",
    "\n",
    "    def __getitem__(self, index: int) -> PPORLElement:\n",
    "        return self.history[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.history)\n",
    "\n",
    "    def create_loader(self, batch_size: int, shuffle: bool) -> DataLoader:\n",
    "        def collate_fn(elems: Iterable[PPORLElement]):\n",
    "            return PPORLBatch(\n",
    "                pad_sequence(\n",
    "                    [elem.query_tensor.flip(0) for elem in elems],\n",
    "                    padding_value=self.pad_token_id,\n",
    "                    batch_first=True,\n",
    "                ).flip(1),\n",
    "                pad_sequence(\n",
    "                    [elem.response_tensor for elem in elems],\n",
    "                    padding_value=self.pad_token_id,\n",
    "                    batch_first=True,\n",
    "                ),\n",
    "                pad_sequence(\n",
    "                    [elem.logprobs for elem in elems],\n",
    "                    padding_value=0.0,\n",
    "                    batch_first=True,\n",
    "                ),\n",
    "                pad_sequence(\n",
    "                    [elem.values for elem in elems],\n",
    "                    padding_value=0.0,\n",
    "                    batch_first=True\n",
    "                ),\n",
    "                pad_sequence(\n",
    "                    [elem.rewards for elem in elems],\n",
    "                    padding_value=0.0,\n",
    "                    batch_first=True,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        return DataLoader(self, batch_size, shuffle=shuffle, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37fedbc-d186-42fc-b20a-ab0947c49783",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Define more PyTorch functions\n",
    "Next, we'll implement more capabilities in PyTorch. This includes:\n",
    "1. A whiten utility\n",
    "2. A GAE utility\n",
    "3. The loss to update the PPO LLM you want to fine-tune\n",
    "4. An Actor class to take steps and experience rewards\n",
    "5. An Agent class to manipulate the Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28d570e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def whiten(x):\n",
    "    var, mean = torch.var_mean(x)\n",
    "    return (x - mean) * torch.rsqrt(var + 1e-8)\n",
    "\n",
    "\n",
    "def gae(\n",
    "    values,\n",
    "    rewards,\n",
    "):\n",
    "    advantages = torch.zeros_like(rewards, device=rewards.device)\n",
    "    last_advantage = 0\n",
    "    last_value = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t in reversed(range(rewards.shape[1])):\n",
    "            delta = rewards[:, t] + config.method.gamma * last_value - values[:, t]\n",
    "            last_advantage = delta + config.method.gamma * config.method.lam * last_advantage\n",
    "            advantages[:, t] = last_advantage\n",
    "            last_value = values[:, t]\n",
    "\n",
    "        returns = advantages + values\n",
    "    \n",
    "    if config.method.use_whitening:\n",
    "        advantages = whiten(advantages)\n",
    "    \n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5575874a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ppo_loss(\n",
    "    logprobs,     \n",
    "    values,       \n",
    "    old_logprobs, \n",
    "    old_values,   \n",
    "    advantages,   \n",
    "    returns,      \n",
    "    mask,         \n",
    "):\n",
    "\n",
    "    values_clipped = torch.clamp(\n",
    "        values,\n",
    "        old_values - config.method.cliprange_value,\n",
    "        old_values + config.method.cliprange_value,\n",
    "    )\n",
    "    \n",
    "    n = mask.sum()\n",
    "    \n",
    "    vf_loss1 = (values - returns) ** 2\n",
    "    vf_loss2 = (values_clipped - returns) ** 2\n",
    "    vf_loss = 0.5 * torch.sum(torch.max(vf_loss1, vf_loss2) * mask) / n\n",
    "\n",
    "    log_ratio = (logprobs - old_logprobs) * mask\n",
    "    ratio = torch.exp(log_ratio)\n",
    "    pg_loss1 = -advantages * ratio\n",
    "    pg_loss2 = -advantages * torch.clamp(ratio, 1.0 - config.method.cliprange, 1.0 + config.method.cliprange)\n",
    "    pg_loss = torch.sum(torch.max(pg_loss1, pg_loss2) * mask) / n\n",
    "    pg_clipfrac = torch.sum((pg_loss2 > pg_loss1).float() * mask) / n\n",
    "\n",
    "    loss = pg_loss + config.method.vf_coef * vf_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "811ddf6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(batch):\n",
    "    model_device = next(model.parameters()).device\n",
    "    query_tensors = batch.query_tensors.to(model_device)\n",
    "    response_tensors = batch.response_tensors.to(model_device)\n",
    "    old_logprobs = batch.logprobs.to(model_device)\n",
    "    old_values = batch.values.to(model_device)\n",
    "    old_rewards = batch.rewards.to(model_device)\n",
    "    \n",
    "    response_length = old_rewards.shape[1]\n",
    "\n",
    "    advantages, returns = gae(old_values, old_rewards)\n",
    "\n",
    "    tokens, attention_mask, position_ids = get_model_inputs(query_tensors, response_tensors, tokenizer.pad_token_id)\n",
    "\n",
    "    logits, values_pred = model(tokens,\n",
    "                                attention_mask=attention_mask,\n",
    "                                position_ids=position_ids)\n",
    "    values_pred = values_pred[:, :-1]\n",
    "    logprobs = logprobs_from_logits(logits[:, :-1, :], tokens[:, 1:])\n",
    "    attention_mask = attention_mask[:, :-1]\n",
    "\n",
    "    start = query_tensors.shape[1] - 1\n",
    "    end = start + response_length\n",
    "    logprobs, values_pred, mask = (\n",
    "        logprobs[:, start:end],\n",
    "        values_pred[:, start:end],\n",
    "        attention_mask[:, start:end],\n",
    "    )\n",
    "\n",
    "    loss = ppo_loss(\n",
    "        logprobs=logprobs,\n",
    "        values=values_pred,\n",
    "        old_logprobs=old_logprobs,\n",
    "        old_values=old_values,\n",
    "        advantages=advantages,\n",
    "        returns=returns,\n",
    "        mask=mask,\n",
    "    )\n",
    "\n",
    "    return loss, old_rewards[:,-1].mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8c337f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Actor():\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            prompt_pipeline,\n",
    "            tokenizer,\n",
    "            chunk_size = 128):\n",
    "        \n",
    "        self.prompt_pipeline = prompt_pipeline\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "        self.prompt_pipeline_loader = self.prompt_pipeline.create_loader(self.chunk_size, shuffle=True)\n",
    "        self.prompt_pipeline_iterator = iter(self.prompt_pipeline_loader)\n",
    "\n",
    "        self.ref_model = Agent(config.model.model_path)\n",
    "        self.ref_model_device = config.train.ref_model_device\n",
    "        self.ref_model = self.ref_model.to(self.ref_model_device)\n",
    "        \n",
    "        self.tokenizer = tokenizer        \n",
    "    \n",
    "\n",
    "    def make_experience(self, model, num_rollouts = 128):\n",
    "        model_device = next(model.parameters()).device\n",
    "        \n",
    "        ppo_rl_elements = []\n",
    "        while len(ppo_rl_elements) < num_rollouts:\n",
    "            try:\n",
    "                batch = next(self.prompt_pipeline_iterator)\n",
    "            except StopIteration:\n",
    "                self.pipeline_iterator = iter(self.prompt_pipeline_loader)\n",
    "                batch = next(self.prompt_pipeline_iterator)\n",
    "            \n",
    "            trajectories = generate(model, self.tokenizer, **batch.to(model_device))\n",
    "\n",
    "            query_tensors = batch.input_ids\n",
    "            response_tensors = trajectories[:, query_tensors.shape[1] :]\n",
    "\n",
    "            all_tokens, attention_mask, position_ids = get_model_inputs(\n",
    "                query_tensors.to(response_tensors.device), response_tensors, self.tokenizer.pad_token_id)\n",
    "            with torch.no_grad():\n",
    "                logits, values = model(\n",
    "                    all_tokens, \n",
    "                    attention_mask=attention_mask, \n",
    "                    position_ids=position_ids)\n",
    "                ref_logits, _ = self.ref_model(\n",
    "                    all_tokens.to(self.ref_model_device),\n",
    "                    attention_mask=attention_mask.to(self.ref_model_device),\n",
    "                    position_ids=position_ids.to(self.ref_model_device))\n",
    "            \n",
    "            all_tokens = all_tokens.cpu()\n",
    "            logits = logits.cpu()\n",
    "            ref_logits = ref_logits.cpu()\n",
    "\n",
    "            logprobs = logprobs_from_logits(logits[:, :-1, :], all_tokens[:, 1:])\n",
    "            ref_logprobs = logprobs_from_logits(ref_logits[:, :-1, :], all_tokens[:, 1:])\n",
    "            \n",
    "            n = trajectories.shape[0]\n",
    "            values = values.cpu()[:, :-1]\n",
    "            query_tensors = query_tensors.cpu()\n",
    "            response_tensors = response_tensors.cpu()\n",
    "            \n",
    "            start = query_tensors.shape[1] - 1\n",
    "            ends = start + attention_mask[:, start:].sum(1)\n",
    "            all_values = [values[i, start : ends[i]] for i in range(n)]\n",
    "            all_logprobs = [logprobs[i, start : ends[i]] for i in range(n)]\n",
    "            \n",
    "            texts = self.tokenizer.batch_decode(trajectories, skip_special_tokens=True)\n",
    "            scores = torch.tensor(reward_fn(texts), device='cpu', dtype=torch.float)\n",
    "\n",
    "            rewards = -config.method.kl_coef * (logprobs - ref_logprobs)\n",
    "            all_rewards = [None] * n\n",
    "            for i in range(n):\n",
    "                rs = rewards[i][start : ends[i]]\n",
    "                rs[-1] = scores[i]\n",
    "                all_rewards[i] = rs\n",
    "            \n",
    "            new_ppo_rl_elements = [\n",
    "                PPORLElement(\n",
    "                    query_tensor=query_tensors[i],\n",
    "                    response_tensor=response_tensors[i],\n",
    "                    logprobs=all_logprobs[i],\n",
    "                    values=all_values[i],\n",
    "                    rewards=all_rewards[i],\n",
    "                )\n",
    "                for i in range(n)\n",
    "            ]\n",
    "\n",
    "            ppo_rl_elements += new_ppo_rl_elements\n",
    "\n",
    "        return ppo_rl_elements, scores.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f50e1af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, input_ids, attention_mask=None, **kwargs):\n",
    "    \n",
    "    generate_kwargs = dict(\n",
    "        config.method.gen_kwargs,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    kwargs = dict(generate_kwargs, **kwargs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_results = model.generate(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "\n",
    "    return generated_results\n",
    "\n",
    "\n",
    "def get_model_inputs(query_tensors, response_tensors, pad_token_id):\n",
    "    tokens = torch.cat((query_tensors, response_tensors), dim=1)[:, -config.train.seq_length :]\n",
    "    attention_mask = (tokens.not_equal(pad_token_id).long().to(tokens.device))\n",
    "    position_ids = attention_mask.cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask.eq(0), 0)\n",
    "    return tokens, attention_mask, position_ids\n",
    "\n",
    "\n",
    "def logprobs_from_logits(logits, labels):\n",
    "    logprobs = F.log_softmax(logits, dim=-1)\n",
    "    logprobs_labels = torch.gather(logprobs, dim=-1, index=labels.unsqueeze(-1))\n",
    "    return logprobs_labels.squeeze(-1)\n",
    "\n",
    "\n",
    "def freeze_bottom_causal_layers(model: nn.Module, num_layers_unfrozen: int = 0):\n",
    "    hidden_layers = model.transformer.h\n",
    "    if num_layers_unfrozen == 0:\n",
    "        hidden_layers_to_freeze = list(hidden_layers)\n",
    "    elif num_layers_unfrozen > 0:\n",
    "        hidden_layers_to_freeze = list(hidden_layers)[:-num_layers_unfrozen]\n",
    "    else:\n",
    "        hidden_layers_to_freeze = []\n",
    "    for layer in hidden_layers_to_freeze:\n",
    "        layer.requires_grad_(False)\n",
    "\n",
    "        \n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, model_path, num_layers_unfrozen=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = transformers.AutoModelForCausalLM.from_pretrained(model_path, cache_dir=\"./models\")\n",
    "\n",
    "        self.logit_head = self.base_model.get_output_embeddings()\n",
    "        \n",
    "        n_embd = self.base_model.lm_head.in_features\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd*2, 1))\n",
    "        \n",
    "        freeze_bottom_causal_layers(self.base_model, num_layers_unfrozen)\n",
    "        \n",
    "    \n",
    "    def generate(self, input_ids, **x):\n",
    "        return self.base_model.generate(input_ids, **x)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, position_ids):\n",
    "\n",
    "        transformer_outputs = self.base_model.transformer(input_ids=input_ids,\n",
    "                                                          attention_mask=attention_mask,\n",
    "                                                          position_ids=position_ids)\n",
    "    \n",
    "        last_hidden_state = transformer_outputs.last_hidden_state\n",
    "        lm_logits = self.logit_head(last_hidden_state)\n",
    "        value = self.value_head(last_hidden_state).squeeze(-1)\n",
    "        \n",
    "        return lm_logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb66aec-62a7-4e1b-9619-a09ecc804b8e",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Define the pipeline, download model and data artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79835e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50b3d1c39b040029413186e5e091ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c41605d0c9849ebb0460435dfd3973c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16c4b2e3af5436cb726117592055615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caceb13c1a2c424899085bf70895fc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2d3d2f86704124a8e9195084c306fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3296619215994df3b8d78c3c0cccee0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_fn = pipeline(\n",
    "    model = \"lvwerra/distilbert-imdb\",\n",
    "    top_k=2,\n",
    "    batch_size=config.method.num_rollouts,\n",
    "    device=config.train.reward_model_device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e696db6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_positive_score(scores):\n",
    "    return dict(map(lambda x: tuple(x.values()), scores))[\"POSITIVE\"]\n",
    "\n",
    "def reward_fn(samples: List[str]) -> List[float]:\n",
    "    sentiments = list(map(get_positive_score, sentiment_fn(samples)))\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "180883d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297765ce2cf5485cb0647f90f45c1a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6395121a9c4c499f2c0094e493233d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883dfdf2d0f041938f14d370bc1858ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263fbc46ec6e46fe8b1bca7ccaf42cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab0b30c06c64b6082c82bbdd6d92db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b24cf68f09f48689d6ab04928ee0adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c98a55f3c14f11a00aca44c10f83c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb = load_dataset(\"imdb\", split=\"train+test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51ad4f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = [\" \".join(review.split()[:config.method.prompt_size]) for review in imdb[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1554f68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83e85cb61564f96920c361bce62be75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/17.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af79664173b34462a33a7a07b8a3bfb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3e32fa7d7141eba5ff0ff21f7997bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e9be37924746a1ab1810f65b6c8af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab43a67b8f94d3db44b1bed29f21d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model.tokenizer_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_ida = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "pad_token_id = 50256\n",
    "\n",
    "max_prompt_length = (config.train.seq_length - config.method.gen_kwargs[\"max_new_tokens\"])\n",
    "test_prompt_pipeline = PromptPipeline(prompts, max_prompt_length, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41f0ad62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ce43c40a6249adaaff455fee1287a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3cfa6a4f14405f87a1679ab7318d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Agent(config.model.model_path, config.model.num_layers_unfrozen).to(config.train.model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c211cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1820,  4203,   546,   262,  3807],\n",
       "        [50256, 50256, 50256,  5661,   318],\n",
       "        [   40,   460,  1560,   351, 18979]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.batch_encode_plus(\n",
    "    [\"my feeling about the movie\", \"this is\", \"I can tell with certainty\"],\n",
    "    return_tensors='pt',\n",
    "    padding=True)['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbba22bb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_device = next(model.parameters()).device\n",
    "output_ids = generate(\n",
    "    model, tokenizer, \n",
    "    input_ids.to(model_device), \n",
    "    max_new_tokens=config.method.gen_kwargs[\"max_new_tokens\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "284f1b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my feeling about the movie, as well as the style and tone of the famous \"Amadeus\" music films.',\n",
       " 'this is just awful! it makes you feel very lonely. it is not until next day Italy that you can see epitomise how bad this film was. i was trying to find something im going down the road to television, whilst watching this I felt humiliated, i totally felt embarrassed i have never felt',\n",
       " 'I can tell with certainty that Pulp, Chicago Voice and I like Victor Oppel that he is correct and his Molly Hallrem was one amazing film --a Mustivar of Terror, and a tremendous piece of exceptional film.On well getting past the Great Affair in Vincent Van Buren, Democracy in Love is the']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1058de6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.988457977771759, 0.006799057591706514, 0.9953449368476868]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_fn(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16efa7d-f12b-4c93-8514-663766dd2aa2",
   "metadata": {},
   "source": [
    "--- \n",
    "### 6. Define the overall training loop and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63696263",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7090c9d0185d4fa4bba546097c1f8b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "prompt_pipeline = PromptPipeline(prompts, config.train.seq_length, tokenizer)\n",
    "\n",
    "actor = Actor(prompt_pipeline, tokenizer, chunk_size=config.method.chunk_size)\n",
    "\n",
    "store = PPORolloutStorage(tokenizer.pad_token_id)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), **config.optimizer.kwargs)\n",
    "scheduler = CosineAnnealingLR(opt, **config.scheduler.kwargs)\n",
    "\n",
    "n_updates_per_batch = config.method.ppo_epochs\n",
    "total_steps = 400 # TODO: fix this\n",
    "\n",
    "tbar = tqdm(initial=0, total=total_steps)\n",
    "\n",
    "for _ in range(config.train.epochs):\n",
    "    \n",
    "    store.clear_history()\n",
    "    rollouts, score = actor.make_experience(model, config.method.num_rollouts)\n",
    "    store.push(rollouts)\n",
    "    train_dataloader = store.create_loader(config.train.batch_size, shuffle=True)\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        for _ in range(n_updates_per_batch):\n",
    "\n",
    "            loss, reward = loss_fn(batch)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            scheduler.step()\n",
    "            tbar.update()\n",
    "    \n",
    "    tbar.set_description(f\"| score: {score:.3f} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c484497-547a-47ce-ad62-196cf369739b",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### 7. Results: Use the updated LLM to generate new text and evaluate with the reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "577f8e73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1820,  4203,   546,   262,  3807],\n",
       "        [50256, 50256, 50256,  5661,   318],\n",
       "        [   40,   460,  1560,   351, 18979]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.batch_encode_plus(\n",
    "    [\"my feeling about the movie\", \"this is\", \"I can tell with certainty\"],\n",
    "    return_tensors='pt',\n",
    "    padding=True)['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3af00680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my feeling about the movie-binding machine was palpable throughout.<br /><br />I am glad I spent more time watching this again, because hopefully I'll watch as many films as I are good at.\n",
      " 0.9671000838279724\n",
      "this is a wonderful film and a gem.\n",
      " 0.9961223006248474\n",
      "I can tell with certainty that this movie has a great deal in common with the Harry Potter trilogy or the Harry Potter films: it's a great treat for kids and adults alike.\n",
      " 0.9945493340492249\n",
      "all rewards mean: 0.9859239061673483\n"
     ]
    }
   ],
   "source": [
    "model_device = next(model.parameters()).device\n",
    "output_ids = generate(\n",
    "    model,       \n",
    "    tokenizer,\n",
    "    input_ids.to(model_device),\n",
    "#     min_length=20,\n",
    "#     max_new_tokens=100,\n",
    "#     do_sample=True,\n",
    "#     top_p=0.92, \n",
    "#     top_k=0\n",
    ")\n",
    "generated_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "rewards = reward_fn(generated_text)\n",
    "print(generated_text[0].replace('\\n', ' ') + '\\n', rewards[0])\n",
    "print(generated_text[1].replace('\\n', ' ') + '\\n', rewards[1])\n",
    "print(generated_text[2].replace('\\n', ' ') + '\\n', rewards[2])\n",
    "print('all rewards mean:',np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a86d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
